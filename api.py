# -*- coding: utf-8 -*-
"""main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_IWTp-ycTSgtu9meW-YPXz0XsOguILNe
"""

import glob
import traceback
import os, uuid, boto3, asyncio, io, subprocess
from fastapi import FastAPI, File, UploadFile, BackgroundTasks, HTTPException
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from test_analyzer import filter_and_track
from court_tracker import CourtTracker 
from tempfile import NamedTemporaryFile
import time

import torch
import cv2
from PIL import Image
from torchvision import models, transforms
from torch import nn
import numpy as np
from io import BytesIO

BUCKET = "test-test-test-test-dd"
PREFIX = "tennis"
OUTPUT_DIR = ""

app = FastAPI()
s3 = boto3.client("s3")

def s3_upload_obj(fileobj, bucket, key):
    s3.upload_fileobj(fileobj, bucket, key)

def s3_download_file(bucket, key, dst_path):
    s3.download_file(bucket, key, dst_path)

def download_s3_range(bucket: str, key: str, start_byte: int, end_byte: int) -> io.BytesIO:
    """
    Grab a byte‑range from S3 and return it as a BytesIO.
    """
    byte_range = f"bytes={start_byte}-{end_byte-1}"
    resp = s3.get_object(Bucket=bucket, Key=key, Range=byte_range)
    data = resp["Body"].read()
    return io.BytesIO(data)

def extract_time_segment(s3_url: str, start: float, duration: float) -> NamedTemporaryFile: # type: ignore
    """
    Ask ffmpeg to open the S3 URL, seek to `start` seconds, read `duration` seconds,
    write it to a temp file, and hand you that temp file path.
    """
    tmp = NamedTemporaryFile(suffix=".mp4", delete=False)
    cmd = [
        "ffmpeg",
        "-ss", str(start),
        "-i", s3_url,
        "-t", str(duration),
        "-c", "copy",            # no re‑encode, just container slice
        tmp.name
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return tmp

court_tracker = CourtTracker("court10.pt")

def is_court(image):
    keypoints = court_tracker.predict(image)
    if keypoints is not None:
        return len(keypoints)==14
    else:
        return False

@app.get("/")
async def read_root():
    return {"message": "🏓 Court-Cam API up and running!"}

@app.post("/upload")
async def upload_video(file: UploadFile = File(...)):
    # 1) Save the incoming UploadFile to disk
    tmp = NamedTemporaryFile(suffix=".mp4", delete=False)
    try:
        data = await file.read()      # read all bytes
        tmp.write(data)
        tmp.flush()
        local_path = tmp.name
    finally:
        tmp.close()

    # 2) Upload that disk file to S3
    rid = str(uuid.uuid4())
    key = f"{PREFIX}/{rid}/{file.filename}"
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(
        None,
        lambda: s3.upload_file(local_path, BUCKET, key)
    )

    # 3) Probe for frame count
    try:
        proc = subprocess.run(
            [
              "/usr/bin/ffprobe", "-v", "error",
              "-count_frames", "-select_streams", "v:0",
              "-show_entries", "stream=nb_read_frames",
              "-of", "default=noprint_wrappers=1:nokey=1",
              local_path
            ],
            capture_output=True, text=True, check=True
        )
        # Capture output and check if there is any error
        output = proc.stdout.strip()
        if output:
            frames = int(output)
            time_est = frames/4.5263
        else:
            frames = None
    except subprocess.CalledProcessError as e:
        print(f"Error calling ffprobe: {e.stderr}")
        frames = None
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        frames = None
    finally:
        os.unlink(local_path)

    return {"s3_key": key, "estimated_time": time_est}

@app.post("/isCourt")
async def is_court_endpoint(file: UploadFile = File(...)):
    contents = await file.read()
    image_array = np.frombuffer(contents, dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)

    if image is None:
        return JSONResponse(content={"error": "Invalid image"}, status_code=400)
    image = cv2.resize(image, (320, 180))
    result = is_court(image)
    return {"is_court": result, "image shape": image.shape}

@app.get("/fetch")
async def fetch_video(key: str):
    '''
    - takes in an s3 key and returns the video
    '''
    # 1) load from S3 in a thread so we don’t block the event loop
    loop = asyncio.get_running_loop()
    try:
        resp = await loop.run_in_executor(
            None,
            lambda: s3.get_object(Bucket=BUCKET, Key=key)
        )
    except s3.exceptions.NoSuchKey:
        raise HTTPException(404, f"No such key: {key}")
    except Exception as e:
        raise HTTPException(500, f"S3 fetch failed: {e}")

    body = resp["Body"]  # this is a botocore.response.StreamingBody

    # 2) stream it back to the caller
    filename = os.path.basename(key)
    headers = {
        "Content-Disposition": f'attachment; filename="{filename}"'
    }
    return StreamingResponse(
        body.iter_chunks(chunk_size=16_384),
        media_type="video/mp4",
        headers=headers
    )

# Allow your mobile app origin (or *) for simplicity
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["POST"],
    allow_headers=["*"],
)

@app.get("/analyze2")
async def analyze_video2(key: str, thresh: float = 0.03, 
                        sample_rate: int = 1, batch_size: int = 30):
    '''
    - uploads video to s3
    - finds the raw upload using key
    - streams per chunk
    - downloads each chunk locally
    - runs process_video() on each chunk
    - saves the result in s3
    - returns a presigned URL to that result & the time it took to process
    '''

    t0 = time.perf_counter()

    # --- locate raw in S3 + download ---
    print("locating in s3")
    rid = str(uuid.uuid4())
    raw = NamedTemporaryFile(suffix=".mp4", delete=False)
    raw.close()
    try:
        s3.download_file(BUCKET, key, raw.name)
    except Exception as e:
        raise HTTPException(500, f"s3 download failed: {e}")

    loop = asyncio.get_running_loop()
    try:
        processed_path, stroke_counts = await loop.run_in_executor(
            None,
            filter_and_track,
            raw.name,
            thresh,
            sample_rate,
            batch_size
        )
    except Exception as e:
        raise HTTPException(500, f"processing failed: {e}")
    
    # upload full video back to s3 for easy access
    print("video combined, uploading to s3")
    out_key = f"{PREFIX}/{rid}/processed_{os.path.basename(key)}"
    await loop.run_in_executor(
        None,
        s3.upload_file,
        processed_path,
        BUCKET,
        out_key
    )

    '''
    # --- upload minimap-only video ---
    minimap_key = f"{PREFIX}/{rid}/minimap_{os.path.basename(key)}"
    await loop.run_in_executor(
        None,
        s3.upload_file,
        minimap_path,
        BUCKET,
        minimap_key
    )
    '''

    # cleanup
    os.remove(raw.name)
    os.remove(processed_path)
    # os.remove(minimap_path)

    print("done! ending timer and returning key")
    elapsed = time.perf_counter() - t0
    return {"processed_s3_key": out_key, 
            # "minimap_s3_key": minimap_key,
            "stroke_counts_per_player": stroke_counts,
            "processing_time_seconds": round(elapsed, 2)}

@app.get("/analyze3")
async def analyze_direct_upload(key: str,
                                thresh: float = 0.3,
                                sample_rate: int = 1,
                                batch_size: int = 30):
    """
    Run filter_and_track, save processed video, and save **all** heatmaps.

    Accepts pipeline returns in any of these shapes:
      - processed_tmp_path
      - (processed_tmp_path, heatmap_tmp_path)
      - (processed_tmp_path, [heatmap_tmp_path, ...])
      - (processed_tmp_path, heatmap_tmp_path_1, heatmap_tmp_path_2, ...)

    Also falls back to discovering ALL new 'heat'/'minimap' videos (mp4/avi/mov/mkv)
    created during this request in OUTPUT_DIR or /tmp.
    """
    t0 = time.perf_counter()
    t_start = time.time()  # for fallback discovery window

    # -------- helpers --------
    def _flatten_heatmap_returns(raw):
        """Normalize any second+ items from the result into a flat list of paths."""
        if raw is None:
            return []
        if isinstance(raw, (str, bytes)):
            return [raw]
        paths = []
        if isinstance(raw, (list, tuple)):
            for item in raw:
                if isinstance(item, (list, tuple)):
                    for sub in item:
                        if isinstance(sub, (str, bytes)):
                            paths.append(sub)
                elif isinstance(item, (str, bytes)):
                    paths.append(item)
        return paths

    def _recent_video_candidates(dirs, earliest_mtime):
        exts = {".mp4", ".avi", ".mov", ".mkv"}
        hits = []
        for d in dirs:
            try:
                for name in os.listdir(d):
                    p = os.path.join(d, name)
                    if not os.path.isfile(p):
                        continue
                    _, ext = os.path.splitext(name)
                    if ext.lower() not in exts:
                        continue
                    low = name.lower()
                    if ("heat" not in low) and ("minimap" not in low):
                        continue
                    try:
                        mtime = os.path.getmtime(p)
                    except Exception:
                        continue
                    if mtime >= (earliest_mtime - 3):  # slack
                        hits.append((mtime, p))
            except FileNotFoundError:
                pass
        hits.sort(key=lambda t: t[0], reverse=True)
        return [p for _, p in hits]

    # --- locate raw in S3 + download ---
    print("locating in s3")
    rid = str(uuid.uuid4())
    raw = NamedTemporaryFile(suffix=".mp4", delete=False)
    raw.close()
    try:
        s3.download_file(BUCKET, key, raw.name)
    except Exception as e:
        raise HTTPException(500, f"s3 download failed: {e}")

    loop = asyncio.get_running_loop()
    try:
        # filter_and_track may return various shapes (see docstring)
        result, stroke_counts = await loop.run_in_executor(
            None, filter_and_track, raw.name, thresh, sample_rate, batch_size
        )
        # Normalize returns
        if isinstance(result, (list, tuple)):
            processed_path = result[0]
            returned_heatmaps = _flatten_heatmap_returns(result[1:])
        else:
            processed_path = result
            returned_heatmaps = []
    except Exception as e:
        print("🔥 Exception in /analyze_upload"); 
        traceback.print_exc()
        raise HTTPException(500, f"processing failed: {e}")
    finally:
        try:
            os.remove(raw.name)
        except Exception:
            pass
    
    out_key = f"{PREFIX}/{rid}/processed_{os.path.basename(key)}"
    await loop.run_in_executor(None, s3.upload_file, processed_path, BUCKET, out_key)
    try:
        os.remove(processed_path)
    except Exception:
        pass
    
    heatmap_s3_keys = []

    for hp in returned_heatmaps:
        try:
            if hp and os.path.exists(hp) and os.path.isfile(hp):
                key = f"{PREFIX}/{rid}/heatmaps/{os.path.basename(hp)}"
                await loop.run_in_executor(None, s3.upload_file, hp, BUCKET, key)
                heatmap_s3_keys.append(key)
                os.remove(hp)
        except Exception:
            traceback.print_exc()

    # -------- fallback: discover ALL likely heatmaps created during this request --------
    if not heatmap_s3_keys:
        discovered = _recent_video_candidates([OUTPUT_DIR, "/tmp"], t_start)
        for p in discovered:
            if os.path.basename(p) == os.path.basename(processed_path):
                continue
            try:
                key = f"{PREFIX}/{rid}/heatmaps/{os.path.basename(p)}"

                await loop.run_in_executor(None, s3.upload_file, p, BUCKET, key)
                heatmap_s3_keys.append(key)
                os.remove(p)
            except Exception:
                traceback.print_exc()

    # -------- legacy fallback: heatmap_*.mp4 patterns (ALL matches) --------
    if not heatmap_s3_keys:
        patterns = []
        for d in (OUTPUT_DIR, "/tmp"):
            patterns.extend(glob.glob(os.path.join(d, "heatmap_*.mp4")))
        for p in sorted(patterns, key=lambda x: os.path.getmtime(x), reverse=True):
            if os.path.basename(p) == os.path.basename(processed_path):
                continue
            try:
                key = f"{PREFIX}/{rid}/heatmaps/{os.path.basename(p)}"

                await loop.run_in_executor(None, s3.upload_file, p, BUCKET, key)
                heatmap_s3_keys.append(key)
                os.remove(p)
            except Exception:
                traceback.print_exc()

    elapsed = round(time.perf_counter() - t0, 2)

    return {
        "video_path": out_key,
        "heatmap_paths": list(dict.fromkeys(heatmap_s3_keys)),
        "heatmap_folder": f"{PREFIX}/{rid}/heatmaps",
        "processing_time_seconds": elapsed,
        "stroke_counts": stroke_counts
    }
